# -*- coding: utf-8 -*-
"""520_Exercise_1_Submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZwpB_kiWXBQOkJBlGUVFknkFQH4JXP5l
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# %pip install evaluate tabulate autopep8 black ast

import os
os.environ["HF_ALLOW_CODE_EVAL"] = "1"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

!pip install autopep8

import black
import autopep8
import ast

def clean_and_format_code(code: str) -> str:

    try:
        formatted = black.format_str(code, mode=black.Mode())
    except black.InvalidInput:
        formatted = code


    if not is_valid_syntax(formatted):
        formatted = autopep8.fix_code(formatted)

    if not is_valid_syntax(formatted):
        return code
    return formatted

def is_valid_syntax(code: str) -> bool:
    try:
        ast.parse(code)
        return True
    except SyntaxError:
        return False

import torch
import gc
from datasets import load_dataset
from evaluate import load
from transformers import AutoModelForCausalLM, AutoTokenizer
from tabulate import tabulate
from tqdm import tqdm

model_label_map = {
    "Solshine/Meta-Llama-3.1-8B-Instruct-Python-Coder": "LLaMa",
    "lmsys/vicuna-13b-v1.5": "Vicuna"
}

model_names = list(model_label_map.keys())
prompt_strategies = ["Chain of Thought", "Self-Debugging"]

all_generations_table = []
results_all = []


for model_name in model_names:
    for prompt_strategy in prompt_strategies:
        code_eval = load("code_eval")

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        model = AutoModelForCausalLM.from_pretrained(model_name).to(device)
        model.eval()

        for i in tqdm(range(10), desc=f"{model_label_map[model_name]} | {prompt_strategy}"):
            dataset = load_dataset("openai_humaneval")['test'].select([i])

            for problem in dataset:
                entry_point = problem['entry_point']
                test_code = problem['test']
                problem_prompt = problem['prompt']

                if prompt_strategy == "Chain of Thought":
                    prompt = f"""
                              \"\"\"
                              Write a clean, compilable Python function named `{entry_point}` that solves the problem below:
                              {problem_prompt}
                              Include your reasoning steps in a docstring, then provide the full working function.
                              \"\"\"
                              """
                elif prompt_strategy == "Self-Debugging":
                    prompt = f"""
                              \"\"\"
                              Write and debug a Python function named `{entry_point}` that solves the following problem:
                              {problem_prompt}
                              Ensure all syntax and imports are valid, indentation correct, and code compilable.
                              \"\"\"
                              """

                problem_candidates = []
                for _ in range(3):
                    inputs = tokenizer(prompt, return_tensors="pt").to(device)
                    input_len = inputs["input_ids"].shape[1]

                    with torch.no_grad():
                        outputs = model.generate(
                            input_ids=inputs["input_ids"],
                            attention_mask=inputs["attention_mask"],
                            max_new_tokens=256,
                            temperature=0.7,
                            top_p=0.95,
                            do_sample=True,
                            eos_token_id=tokenizer.eos_token_id
                        )

                    generated_ids = outputs[0][input_len:]
                    raw_code = tokenizer.decode(generated_ids, skip_special_tokens=True)
                    generated_code = clean_and_format_code(raw_code)
                    problem_candidates.append(generated_code)

                all_generations_table.append({
                    "Problem ID": i + 1,
                    "LLM": model_label_map[model_name],
                    "Prompt Type": prompt_strategy,
                    "Gen 1": problem_candidates[0],
                    "Gen 2": problem_candidates[1],
                    "Gen 3": problem_candidates[2]
                })

                k_values = [1, 3]
                pass_at_k, results = code_eval.compute(
                    references=[test_code],
                    predictions=[problem_candidates],
                    k=k_values,
                    num_workers=2,
                    timeout=10.0
                )

                results_all.append({
                    "LLM": model_label_map[model_name],
                    "Prompt Type": prompt_strategy,
                    "Problem ID": i + 1,
                    "Pass@1": round(pass_at_k["pass@1"] * 100, 2),
                    "Pass@3": round(pass_at_k["pass@3"] * 100, 2),
                    "RawResults": results
                })

        del model, tokenizer
        torch.cuda.empty_cache()
        gc.collect()
        torch.cuda.ipc_collect()

all_generations = []

for entry in all_generations_table:
    all_generations.append(entry.get('Gen 1', ''))
    all_generations.append(entry.get('Gen 2', ''))
    all_generations.append(entry.get('Gen 3', ''))


all_text = '\n\n---\n\n'.join(all_generations)

with open('all_generations_cleaned_prompt.txt', 'w', encoding='utf-8') as f:
    f.write(all_text)

from google.colab import files
files.download("/content/all_generations_cleaned_prompt.txt")

from tabulate import tabulate

print("\n=== All Generated Code Samples ===")
table_str = tabulate(all_generations_table, headers="keys", tablefmt="grid")
print(table_str)

with open("all_generated_code_samples_cleaned_prompt.csv", "w", encoding="utf-8") as f:
    f.write(table_str)

from google.colab import files
files.download("/content/all_generated_code_samples_cleaned_prompt.csv")

from tabulate import tabulate

detailed_rows = []
for gen_entry in all_generations_table:
    result_entry = next(
        (r for r in results_all
         if r["LLM"] == gen_entry["LLM"]
         and r["Prompt Type"] == gen_entry["Prompt Type"]
         and r["Problem ID"] == gen_entry["Problem ID"]),
        None
    )
    if result_entry:
        detailed_rows.append({
            "Problem ID": gen_entry["Problem ID"],
            "LLM": gen_entry["LLM"],
            "Prompt Type": gen_entry["Prompt Type"],
            "Gen 1": gen_entry["Gen 1"].strip()[:80] + "...",
            "Gen 2": gen_entry["Gen 2"].strip()[:80] + "...",
            "Gen 3": gen_entry["Gen 3"].strip()[:80] + "...",
            "Pass@1": result_entry["Pass@1"],
            "Pass@3": result_entry["Pass@3"],
            "RawResults": result_entry["RawResults"][0]
        })

table_pass_str = tabulate(detailed_rows, headers="keys", tablefmt="grid", showindex=False)

with open("pass_results_cleaned_prompt.csv", "w", encoding="utf-8") as f:
    f.write(table_pass_str)

from google.colab import files
files.download("/content/pass_results_cleaned_prompt.csv")
